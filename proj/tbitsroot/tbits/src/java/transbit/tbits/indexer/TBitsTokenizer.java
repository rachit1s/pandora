/*
 * Copyright (c) 2005 Transbit Technologies Pvt. Ltd. All rights reserved.
 *
 * This software is the confidential and proprietary information
 * of Transbit Technologies Pvt. Ltd. ("Confidential Information").  You
 * shall not disclose such Confidential Information and shall use
 * it only in accordance with the terms of the license agreement
 * you entered into with Transbit Technologies Pvt. Ltd.
 */



/* Generated By:JavaCC: Do not edit this line. TBitsTokenizer.java */

/**
 * TBitsTokenizer.java
 *
 * $Header:
 */
package transbit.tbits.indexer;

/**
 * A grammar-based tokenizer constructed with JavaCC.
 */
public class TBitsTokenizer extends org.apache.lucene.analysis.Tokenizer implements TBitsTokenizerConstants {
    static private int[] jj_la1_0;

    //~--- static initializers ------------------------------------------------

    static {
        jj_la1_0();
    }

    //~--- fields -------------------------------------------------------------

    final private int[]               jj_la1        = new int[1];
    private int                       jj_kind       = -1;
    private java.util.Vector          jj_expentries = new java.util.Vector();
    private int[]                     jj_expentry;
    private int                       jj_gen;
    private int                       jj_ntk;
    public Token                      token, jj_nt;
    public TBitsTokenizerTokenManager token_source;

    //~--- constructors -------------------------------------------------------

    public TBitsTokenizer(CharStream stream) {
        token_source = new TBitsTokenizerTokenManager(stream);
        token        = new Token();
        jj_ntk       = -1;
        jj_gen       = 0;

        for (int i = 0; i < 1; i++) {
            jj_la1[i] = -1;
        }
    }

    public TBitsTokenizer(java.io.Reader reader) {
        this(new FastCharStream(reader));
        this.input = reader;
    }

    public TBitsTokenizer(TBitsTokenizerTokenManager tm) {
        token_source = tm;
        token        = new Token();
        jj_ntk       = -1;
        jj_gen       = 0;

        for (int i = 0; i < 1; i++) {
            jj_la1[i] = -1;
        }
    }

    //~--- methods ------------------------------------------------------------

    public void ReInit(CharStream stream) {
        token_source.ReInit(stream);
        token  = new Token();
        jj_ntk = -1;
        jj_gen = 0;

        for (int i = 0; i < 1; i++) {
            jj_la1[i] = -1;
        }
    }

    public void ReInit(TBitsTokenizerTokenManager tm) {
        token_source = tm;
        token        = new Token();
        jj_ntk       = -1;
        jj_gen       = 0;

        for (int i = 0; i < 1; i++) {
            jj_la1[i] = -1;
        }
    }

    final public void disable_tracing() {}

    final public void enable_tracing() {}

    @SuppressWarnings("unchecked")
    public ParseException generateParseException() {
        jj_expentries.removeAllElements();

        boolean[] la1tokens = new boolean[18];

        for (int i = 0; i < 18; i++) {
            la1tokens[i] = false;
        }

        if (jj_kind >= 0) {
            la1tokens[jj_kind] = true;
            jj_kind            = -1;
        }

        for (int i = 0; i < 1; i++) {
            if (jj_la1[i] == jj_gen) {
                for (int j = 0; j < 32; j++) {
                    if ((jj_la1_0[i] & (1 << j)) != 0) {
                        la1tokens[j] = true;
                    }
                }
            }
        }

        for (int i = 0; i < 18; i++) {
            if (la1tokens[i]) {
                jj_expentry    = new int[1];
                jj_expentry[0] = i;
                jj_expentries.addElement(jj_expentry);
            }
        }

        int[][] exptokseq = new int[jj_expentries.size()][];

        for (int i = 0; i < jj_expentries.size(); i++) {
            exptokseq[i] = (int[]) jj_expentries.elementAt(i);
        }

        return new ParseException(token, exptokseq, tokenImage);
    }

    final private Token jj_consume_token(int kind) throws ParseException {
        Token oldToken;

        if ((oldToken = token).next != null) {
            token = token.next;
        } else {
            token = token.next = token_source.getNextToken();
        }

        jj_ntk = -1;

        if (token.kind == kind) {
            jj_gen++;

            return token;
        }

        token   = oldToken;
        jj_kind = kind;

        throw generateParseException();
    }

    private static void jj_la1_0() {
        jj_la1_0 = new int[] { 0x87ff, };
    }

    final private int jj_ntk() {
        if ((jj_nt = token.next) == null) {
            return (jj_ntk = (token.next = token_source.getNextToken()).kind);
        } else {
            return (jj_ntk = jj_nt.kind);
        }
    }

    /**
     * Returns the next token in the stream, or null at EOS.
     * <p>The returned token's type is set to an element of {@link
     * TBitsTokenizerConstants#tokenImage}.
     */
    final public org.apache.lucene.analysis.Token next() throws ParseException, java.io.IOException {
        Token token = null;

        switch ((jj_ntk == -1)
                ? jj_ntk()
                : jj_ntk) {
        case ALPHANUM :
            token = jj_consume_token(ALPHANUM);

            break;

        case APOSTROPHE :
            token = jj_consume_token(APOSTROPHE);

            break;

        case ACRONYM :
            token = jj_consume_token(ACRONYM);

            break;

        case COMPANY :
            token = jj_consume_token(COMPANY);

            break;

        case EMAIL :
            token = jj_consume_token(EMAIL);

            break;

        case HOST :
            token = jj_consume_token(HOST);

            break;

        case SMARTLINK :
            token = jj_consume_token(SMARTLINK);

            break;

        case PERLMOD :
            token = jj_consume_token(PERLMOD);

            break;

        case FILENAME :
            token = jj_consume_token(FILENAME);

            break;

        case NUM :

            // token = <FILEPATH> |
            // token = <DOSFILEPATH> |
            // token = <UNCPATH> |
            token = jj_consume_token(NUM);

            break;

        case CJK :
            token = jj_consume_token(CJK);

            break;

        case 0 :
            token = jj_consume_token(0);

            break;

        default :
            jj_la1[0] = jj_gen;
            jj_consume_token(-1);

            throw new ParseException();
        }

        if (token.kind == EOF) {
            {
                if (true) {
                    return null;
                }
            }
        } else {
            {
                if (true) {
                    return new org.apache.lucene.analysis.Token(token.image, token.beginColumn, token.endColumn, tokenImage[token.kind]);
                }
            }
        }

        throw new Error("Missing return statement in function");
    }

    //~--- get methods --------------------------------------------------------

    final public Token getNextToken() {
        if (token.next != null) {
            token = token.next;
        } else {
            token = token.next = token_source.getNextToken();
        }

        jj_ntk = -1;
        jj_gen++;

        return token;
    }

    final public Token getToken(int index) {
        Token t = token;

        for (int i = 0; i < index; i++) {
            if (t.next != null) {
                t = t.next;
            } else {
                t = t.next = token_source.getNextToken();
            }
        }

        return t;
    }
}
